{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hfy2WbOh4Dyq"
      },
      "outputs": [],
      "source": [
        "!pip install upstash-vector\n",
        "!pip install openai\n",
        "\n",
        "!pip install tiktoken\n",
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dependencies"
      ],
      "metadata": {
        "id": "WvVQSZikoods"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from upstash_vector import Index\n",
        "\n",
        "index = Index(url=\"https://XXXXX-vector.upstash.io\", token=\"XXXX\")\n",
        "# index.upsert(\n",
        "#   vectors=[\n",
        "#       (\"id1\", [0.28, 0.05, 0.06, 0.03, 0.1, 0.9, 0.38, 0.52, 0.9, 0.56, 0.89, 0.3, 0.06, 0.96, 0.92, 0, 0.78, 0.82, 0.37, 0.13, 0.5, 0.11, 0.56, 0.85, 0.72, 0.39, 0.87, 0.53, 0.74, 0.67, 0.59, 1, 0.94, 0.62, 0.06, 0.24, 0.44, 0.16, 0.91, 0.9, 0.78, 0.72, 0.74, 0.7, 0.35, 0.34, 0.78, 0.11, 0.65, 0.26, 0.09, 0.67, 0.51, 0.3, 0.65, 0.58, 0.89, 0.01, 0.32, 0.06, 0.39, 0.02, 0.29, 0.11, 0.81, 0.37, 0.63, 0.18, 0.83, 0.5, 0.34, 0.44, 0.8, 0.56, 0.57, 0.98, 0.39, 0.87, 0.72, 0.72, 0.87, 0.13, 0.97, 0.83, 0.84, 0.8, 0.62, 0.13, 0.46, 0.94, 0.88, 0.3, 0.96, 0.06, 0.43, 0.47, 0.73, 0.95, 0.6, 0.93, 0.97, 0.08, 0.02, 0.6, 0.1, 0.26, 0.38, 0.64, 0.54, 0.56, 0.21, 0.7, 0.58, 0.42, 0.38, 0.51, 0.11, 0.91, 0.99, 0.9, 0.11, 0.42, 0.52, 0.6, 0.32, 0.42, 0.49, 0.69, 0.11, 0.7, 0.29, 0.6, 0.29, 0.99, 0.06, 0.75, 0.35, 0.26, 0.56, 0.67, 0.36, 0.16, 0.87, 0.61, 0.4, 0.6, 0.4, 0.47, 0.82, 0.06, 0.27, 0.65, 0.1, 0.62, 0.24, 0.64, 0.1, 0.99, 0.35, 0.63, 0.26, 0.47, 0.57, 0.63, 0.77, 0.41, 0.53, 0.35, 0.55, 0.57, 0.16, 0.23, 0.08, 0.28, 0.42, 0.94, 0.69, 0.62, 0.21, 0.86, 0.1, 0.57, 0.96, 0.49, 0.11, 0.19, 0.91, 0.92, 0.87, 0.18, 0.07, 0.14, 0.02, 0.71, 0.17, 0.92, 0.87, 0.7, 0.45, 0.98, 0.56, 0.66, 0.76, 0.7, 0.09, 0.75, 0.81, 0.55, 0.87, 0.39, 0.85, 0.98, 0.21, 0.1, 0.38, 0.44, 0.89, 0.83, 0.28, 0.05, 0.22, 0.38, 0.69, 0.26, 0.5, 0.85, 0.58, 0.21, 0.56, 0.39, 0.54, 0.47, 0.58, 0.53, 0.63, 0.65, 0.99, 0.18, 0.78, 0.18, 0.81, 0.8, 0.84, 0.35, 0.27, 0.76, 0.06, 0.25, 0.85, 0.74, 0.12, 0.77, 0.55, 0.83, 0.88, 0.04, 0.88, 0.5, 0.36, 0.56, 0.61, 0.85, 0.55, 0.61, 0.78, 0.53, 0.67, 0.8, 0.29, 0.38, 0.49, 0.96, 0.99, 0.65, 0.66, 0.87, 0.67, 0.05, 0.62, 0.87, 0.78, 0.8, 0.9, 0.14, 0.45, 0.55, 0.8, 0.64, 0.21, 0.18, 0.49, 0.41, 0.45, 0.12, 0.8, 0.13, 0.1, 0.39, 0.16, 0.7, 0.51, 0.31, 0.96, 0.94, 0.11, 0.44, 0.91, 0.87, 0.05, 0.08, 0.78, 0.86, 0.49, 0.23, 0.12, 0.15, 0.5, 0.96, 0.35, 0.02, 0.3, 0.24, 0.49, 0.59, 0.7, 0.89, 0.95, 0.33, 0.26, 0.8, 0.12, 0.78, 0.58, 0.91, 0.25, 0.3, 0.17, 0.84, 0.04, 0.57, 0.33, 0.64, 0.39, 0.26, 0.17, 0.4, 0.29, 0.47, 0.15, 0.55, 0.76, 0.9, 0.48, 0.22, 0.39, 0.76, 0.85, 0.55, 0.97, 0.44, 0.21, 0.4, 0.8, 0.59, 0.87, 0.12, 0.41, 0.59, 0.16, 0.81, 0.65, 0.22, 0.78, 0.06, 0.49, 0.19, 0.73, 0.2, 0.05, 0.73, 0.11, 0.09, 0.29, 0.71, 0.63, 0.16, 0.62, 0.29, 0.12, 0.06, 0.36, 0.13, 0.13, 0.3, 0, 0.7, 0.53, 0.54, 0.81, 0.15, 0.29, 0.65, 0.21, 0.63, 0.45, 0.83, 0.74, 0.74, 0.93, 0.81, 0.35, 0.05, 0.25, 0.41, 0.64, 0.65, 0.62, 0.05, 0.65, 0.83, 0.65, 0.84, 0.6, 0.94, 0.81, 0.72, 0.91, 0.92, 0.78, 0.74, 0.51, 0.96, 0.31, 0.54, 0.23, 0.05, 0.42, 0.47, 0.32, 0.72, 0.69, 0.69, 0.24, 0.3, 0.19, 0.36, 0.58, 0.11, 0.57, 0.97, 0.28, 0.27, 0.88, 0.17, 0.74, 0.34, 0.83, 0.89, 0.42, 0.47, 0.79, 0.51, 0.88, 0.33, 0.85, 0.9, 0.95, 0.72, 0.74, 0.84, 0.88, 0.03, 0.13, 0.49, 0.61, 0.63, 0.24, 0.33, 0.62, 0.21, 0.2, 0.2, 0.49, 0.07, 0.06, 0.36, 0.2, 0.75, 0.56, 0.11, 0.59, 0.21, 0.29, 0.5, 0.02, 0.42, 0.12, 0.65, 0.53, 0.72, 0.72, 0.29, 0.35, 0.51, 0.79, 0.94, 0.26, 0.81, 0.46, 0.83, 0.6, 0.97, 0.81, 0.57, 0.87, 0.04, 0.65, 0.37, 0.32, 0.11, 0.58, 0.78, 0.57, 0.33, 0.43, 0.81, 0.14, 0.38, 0.81, 0.79, 0.7, 0.79, 0.59, 0.5, 0.23, 0.41, 0.27, 0.53, 0.71, 0.93, 0.21, 0.17, 0.64, 0.9, 0.12, 0.03, 0.16, 0.04, 0.8, 0.07, 0.93, 0.26, 0.6, 0.56, 0.43, 0.85, 0.39, 0.18, 0.67, 0.06, 0.99, 0.81, 0.68, 0.62, 0.1, 0.66, 0.95, 0.09, 0.03, 0.01, 0.4, 0.37, 0.36, 0.83, 0.12, 0, 0.45, 0.16, 0.09, 0.84, 0.22, 0.53, 0.6, 0.66, 0.02, 0.03, 0.17, 0.54, 0.55, 0.21, 0.27, 0.38, 0.49, 0.54, 0.5, 0.31, 0.27, 0.75, 0.62, 0.93, 0.59, 0.59, 0.3, 0.51, 0.48, 0.14, 0.45, 0.27, 0.69, 0.36, 0.18, 0.75, 0.3, 0.44, 0.47, 0.14, 0.34, 0.42, 0.71, 0.42, 0.78, 0.43, 0.83, 0.73, 0.31, 0.44, 0.79, 0.08, 0.67, 0.62, 0.09, 0.64, 0.53, 0.45, 0.08, 0.47, 0.28, 0.43, 0.45, 0.28, 0.97, 0.04, 0.42, 0.54, 0.74, 0.85, 0.12, 0.8, 0.84, 0.11, 0.71, 0.84, 0.35, 0.27, 0.17, 0.25, 0.98, 0.05, 0.06, 0.87, 0.47, 0.53, 0.28, 0.26, 0.36, 0.5, 0.66, 0.32, 0.64, 0.1, 0.27, 0.85, 0.43, 0.87, 0.99, 0.99, 0.42, 0.07, 0.53, 0.48, 0.54, 0.27, 0.23, 0.61, 0.11, 0.51, 0.83, 0.01, 0.74, 0.15, 0.8, 0.47, 0.59, 0.01, 0.57, 0.01, 0.48, 0.14, 0.33, 0.9, 0.7, 0.45, 0.68, 0.25, 0.28, 0.25, 0.28, 0.1, 0.69, 0.63, 0.18, 0.34, 0.4, 0.23, 0.13, 0.38, 0.73, 0.78, 0.98, 0.57, 0.92, 0.46, 0.22, 0.94, 0.75, 0.53, 0.6, 0.24, 0.5, 0.3, 1, 0.6, 0.16, 0.78, 0.22, 0.41, 0.33, 0.54, 0.3, 0.42, 0.45, 0.25, 0.45, 0.78, 0.25, 0, 0.11, 0.11, 0.15, 0.88, 0.92, 0.89, 0.48, 0.56, 0.86, 0.1, 0.37, 0.85, 0.8, 0.53, 0.05, 0.52, 0.89, 0.07, 0.49, 0.03, 0.55, 0.51, 0.74, 0.94, 0.78, 0.33, 0.94, 0.23, 0.9, 0.28, 0.34, 0.47, 0.53, 0.71, 0.73, 0.87, 0.76, 0.41, 0.85, 0.33, 0.43, 0.98, 0.19, 0.31, 0.76, 0.21, 0.9, 0.61, 0.51, 0.37, 0.6, 0.4, 0.08, 0.85, 0.47, 0.62, 0.3, 0.49, 0.97, 0.5, 0.93, 0.43, 0.35, 0.62, 0.17, 0.49, 0.59, 0.73, 0.5, 0.85, 0.08, 0.42, 0.85, 0.14, 0.7, 0.51, 0.06, 0.08, 0.53, 0.35, 0.48, 0.84, 0.82, 0.19, 0.43, 0.93, 0.18, 0.52, 0.49, 0.52, 0.09, 0.46, 0.48, 0.45, 0.8, 0.32, 0.88, 0.39, 0.24, 0.37, 0.56, 0.92, 0.75, 0, 0.82, 0.61, 0.33, 0.01, 0.55, 0.18, 0.16, 0.21, 0.56, 0.49, 0.69, 0.52, 0.45, 0.78, 0.17, 0.7, 0.14, 0.57, 0.28, 0.19, 0.15, 0.49, 0.57, 0.32, 0.54, 0.07, 0.81, 0.51, 0.14, 0.08, 0.28, 0.29, 0.24, 0.3, 0.4, 0.82, 0.49, 0.68, 0.48, 0.25, 0.22, 0.58, 0.54, 0.06, 0.97, 0.8, 0.21, 0.53, 0.36, 0.93, 0.54, 0.91, 0.03, 0.04, 0.18, 0.05, 0.97, 0.59, 0.74, 0.09, 0.43, 0.18, 0.33, 0.21, 0.68, 0.4, 0.49, 0.02, 0.31, 0.68, 0.9, 0.37, 0.63, 0.51, 0.65, 0.08, 0.55, 0.61, 0.54, 0.64, 0.92, 0.12, 0.89, 0.21, 0.29, 0.93, 0.86, 0.01, 0.65, 0.77, 0.05, 0.98, 0.72, 0.25, 0.24, 0.41, 0.31, 0.42, 0.62, 0.14, 0.62, 0.45, 0.24, 0.42, 0.59, 0.6, 0.9, 0.16, 0.9, 0.93, 0.16, 0.57, 0.57, 0.31, 0.16, 0.16, 0.03, 0.13, 0.98, 0.47, 0.68, 0.61, 0.28, 0.95, 0.37, 0.62, 0.97, 0.7, 0.29, 0.84, 0.99, 0.06, 0.03, 0.64, 0.43, 0.3, 0.17, 0.08, 0.25, 0.53, 0.95, 0.85, 0.81, 0.01, 0.58, 0.96, 0.56, 0.75, 0.83, 0.17, 0.04, 0.32, 0.67, 0.55, 1, 0.86, 0.22, 0.21, 0.35, 0.56, 0.23, 0.43, 0.87, 0.46, 0.68, 0.15, 0.99, 0.17, 0.46, 0.45, 0.11, 0.6, 0.05, 0.58, 0.93, 0.21, 0.54, 0.14, 0.71, 0.1, 0.98, 0.38, 0.1, 0.46, 0.19, 0.83, 0.31, 0.35, 0.13, 0.12, 1, 0.6, 0.51, 0.33, 0.77, 0.64, 0.55, 0.09, 0.7, 0.02, 0.22, 0.22, 0.93, 0.16, 0.28, 0.93, 0.03, 0.65, 0.18, 0.07, 0.35, 0.39, 0.19, 0.66, 0.65, 0.99, 0.65, 0.66, 0.35, 0.32, 0.98, 0.93, 0.88, 0.51, 0.85, 0.94, 0.79, 0.85, 0.22, 0.06, 0.99, 0.07, 0.15, 0.68, 0.56, 0.55, 0.8, 0.73, 0.39, 0.31, 0.75, 0.6, 0.33, 0.32, 0.51, 0.03, 0.18, 0.81, 0.23, 0.87, 0.66, 0.19, 0.96, 0.31, 0.02, 0.42, 0.53, 0.08, 0.49, 0.19, 0.47, 0.32, 0.11, 0.11, 0.59, 0.12, 0.38, 0.56, 0.4, 0.51, 0.23, 0.53, 0.17, 0.35, 0.6, 0.45, 0.15, 0.29, 0.76, 0.07, 0.54, 0.12, 0.46, 0.68, 0.02, 0.08, 0.36, 0.54, 0.91, 0.58, 0.13, 0.77, 0.34, 0.4, 0.23, 0.44, 0.39, 0.14, 0.01, 0.88, 0.42, 0.48, 0.6, 0.91, 0.48, 0.96, 0.17, 0.37, 0.07, 0.36, 0.73, 0.38, 0.47, 0.98, 0.69, 0.09, 0.53, 0.02, 0.95, 0.77, 0.47, 0.31, 0.49, 0.74, 0.25, 0.04, 0.56, 0.29, 0.56, 0.48, 0.81, 0.03, 0.87, 0.15, 0.43, 0.43, 0.63, 0.3, 0.95, 0.66, 0.41, 0.53, 0.47, 0.31, 0.32, 0.98, 0.19, 0.73, 0.22, 0.94, 0.27, 0.98, 0.84, 0.32, 0.67, 0.5, 0.58, 0.72, 0.42, 0.85, 0.25, 0.31, 0.2, 0.75, 0.16, 0.89, 0.48, 0.75, 0.77, 0.31, 0.95, 0.55, 0.21, 0.66, 0.34, 0.62, 0.59, 0.56, 0.86, 0.23, 0.38, 0.64, 0.43, 0.84, 0.05, 0.29, 0.6, 0.31, 0.94, 0.5, 0.32, 0.1, 0.25, 0.78, 0.66, 0.67, 0.7, 0.5, 0.48, 0.8, 0.37, 0.85, 0.53, 0.61, 0.86, 0.8, 0.89, 0.22, 0.45, 0.53, 0.62, 0.16, 0.99, 0.1, 0.43, 0.92, 0.66, 0.8, 0.2, 0.03, 0.26, 0.07, 0.21, 0.83, 0.6, 0.09, 0.15, 0.67, 0.99, 0.03, 0.05, 0.52, 0.45, 0, 0.37, 0.11, 1, 0.1, 0.14, 0.84, 0.37, 0.55, 0.53, 0.76, 0.16, 0.6, 0.13, 0.03, 0.78, 0.89, 0.91, 0.13, 0.61, 0.49, 0.23, 0.6, 0.67, 0.53, 0.85, 0.41, 0.66, 0.05, 0.98, 0.38, 0.89, 0.45, 0.19, 0.32, 0.46, 0.58, 0.67, 0.33, 0.07, 0.12, 0.32, 0.94, 0.88, 0.85, 0.63, 0.89, 0.82, 0.82, 0.19, 0.03, 0.69, 0.01, 0.17, 0.47, 0.17, 0.01, 0.12, 0.22, 0.74, 0.67, 0.25, 0.84, 0.04, 0.87, 0.61, 0.03, 0.68, 0.22, 0.8, 0.57, 0.27, 0.31, 0.54, 0.76, 0.61, 0.4, 0.33, 0.11, 0.61, 0.89, 0.56, 0.45, 0.67, 0.06, 0.04, 0.29, 0.06, 0.39, 0.92, 0.87, 0.17, 0.94, 0.93, 0.21, 0.09, 0.52, 0.83, 0.57, 0.89, 0.24, 0.91, 0.69, 0.39, 0.57, 0.71, 0.09, 0.64, 0.03, 0.44, 0.05, 0.31, 0.76, 0.34, 0.81, 0.67, 0.9, 0.56, 0.75, 0.01, 0.54, 0.35, 0.63, 0.15, 0.31, 0.9, 0.26, 0.32, 0.23, 0.18, 0.51, 0.96, 0.51, 0.05, 0.07, 0.39, 0.39, 0.22, 0.63, 0.51, 0.2, 0.43, 0.74, 0.13, 0.88, 0.91, 0.12, 0.46, 0.33, 0.65, 0.63, 0.52, 0.84, 0.83, 0.65, 0.64, 0.76, 0.05, 0.67, 0.62, 0.71, 0.33, 0.47, 0.77, 0.75, 0.05, 0.33, 0.1, 0.38, 0.68, 0.29, 0.81, 0.61, 0.39, 0.19, 0.03, 0.53, 0.53, 0.36, 0.08, 0.5, 0.21, 0.93, 0.39, 0.5, 0.07, 0.32, 0.57, 0.24, 0.82, 0.74, 0.26, 0.73, 0.34, 0.62, 0.91, 0.24, 0.03, 0.22, 0.69, 0.89, 0.7, 0.94, 0.14, 0.52, 0.72, 0.47, 0.89, 0.33, 0.37, 0.22, 0.62, 0.57, 0.28, 0.24, 0.6, 0.75, 0.19, 0.55, 0.35, 0.27, 0.02, 0.46, 0.6, 0.95, 0.59, 0.26, 0.13, 0.14, 0.79, 0.37, 0.41, 0.14, 0.42, 0.56, 0.94, 0.13, 0.63, 0.37, 0.35, 0.28, 0.96, 0.38, 0.98, 0.76, 0.81, 0.22, 0.2, 0.33, 0.27, 0.86, 0.63, 0.33], {\"metadata_field\": \"metadata_value\"}),\n",
        "#   ]\n",
        "# )\n",
        "\n",
        "# index.query(\n",
        "#   vector=[0.28, 0.05, 0.06, 0.03, 0.1, 0.9, 0.38, 0.52, 0.9, 0.56, 0.89, 0.3, 0.06, 0.96, 0.92, 0, 0.78, 0.82, 0.37, 0.13, 0.5, 0.11, 0.56, 0.85, 0.72, 0.39, 0.87, 0.53, 0.74, 0.67, 0.59, 1, 0.94, 0.62, 0.06, 0.24, 0.44, 0.16, 0.91, 0.9, 0.78, 0.72, 0.74, 0.7, 0.35, 0.34, 0.78, 0.11, 0.65, 0.26, 0.09, 0.67, 0.51, 0.3, 0.65, 0.58, 0.89, 0.01, 0.32, 0.06, 0.39, 0.02, 0.29, 0.11, 0.81, 0.37, 0.63, 0.18, 0.83, 0.5, 0.34, 0.44, 0.8, 0.56, 0.57, 0.98, 0.39, 0.87, 0.72, 0.72, 0.87, 0.13, 0.97, 0.83, 0.84, 0.8, 0.62, 0.13, 0.46, 0.94, 0.88, 0.3, 0.96, 0.06, 0.43, 0.47, 0.73, 0.95, 0.6, 0.93, 0.97, 0.08, 0.02, 0.6, 0.1, 0.26, 0.38, 0.64, 0.54, 0.56, 0.21, 0.7, 0.58, 0.42, 0.38, 0.51, 0.11, 0.91, 0.99, 0.9, 0.11, 0.42, 0.52, 0.6, 0.32, 0.42, 0.49, 0.69, 0.11, 0.7, 0.29, 0.6, 0.29, 0.99, 0.06, 0.75, 0.35, 0.26, 0.56, 0.67, 0.36, 0.16, 0.87, 0.61, 0.4, 0.6, 0.4, 0.47, 0.82, 0.06, 0.27, 0.65, 0.1, 0.62, 0.24, 0.64, 0.1, 0.99, 0.35, 0.63, 0.26, 0.47, 0.57, 0.63, 0.77, 0.41, 0.53, 0.35, 0.55, 0.57, 0.16, 0.23, 0.08, 0.28, 0.42, 0.94, 0.69, 0.62, 0.21, 0.86, 0.1, 0.57, 0.96, 0.49, 0.11, 0.19, 0.91, 0.92, 0.87, 0.18, 0.07, 0.14, 0.02, 0.71, 0.17, 0.92, 0.87, 0.7, 0.45, 0.98, 0.56, 0.66, 0.76, 0.7, 0.09, 0.75, 0.81, 0.55, 0.87, 0.39, 0.85, 0.98, 0.21, 0.1, 0.38, 0.44, 0.89, 0.83, 0.28, 0.05, 0.22, 0.38, 0.69, 0.26, 0.5, 0.85, 0.58, 0.21, 0.56, 0.39, 0.54, 0.47, 0.58, 0.53, 0.63, 0.65, 0.99, 0.18, 0.78, 0.18, 0.81, 0.8, 0.84, 0.35, 0.27, 0.76, 0.06, 0.25, 0.85, 0.74, 0.12, 0.77, 0.55, 0.83, 0.88, 0.04, 0.88, 0.5, 0.36, 0.56, 0.61, 0.85, 0.55, 0.61, 0.78, 0.53, 0.67, 0.8, 0.29, 0.38, 0.49, 0.96, 0.99, 0.65, 0.66, 0.87, 0.67, 0.05, 0.62, 0.87, 0.78, 0.8, 0.9, 0.14, 0.45, 0.55, 0.8, 0.64, 0.21, 0.18, 0.49, 0.41, 0.45, 0.12, 0.8, 0.13, 0.1, 0.39, 0.16, 0.7, 0.51, 0.31, 0.96, 0.94, 0.11, 0.44, 0.91, 0.87, 0.05, 0.08, 0.78, 0.86, 0.49, 0.23, 0.12, 0.15, 0.5, 0.96, 0.35, 0.02, 0.3, 0.24, 0.49, 0.59, 0.7, 0.89, 0.95, 0.33, 0.26, 0.8, 0.12, 0.78, 0.58, 0.91, 0.25, 0.3, 0.17, 0.84, 0.04, 0.57, 0.33, 0.64, 0.39, 0.26, 0.17, 0.4, 0.29, 0.47, 0.15, 0.55, 0.76, 0.9, 0.48, 0.22, 0.39, 0.76, 0.85, 0.55, 0.97, 0.44, 0.21, 0.4, 0.8, 0.59, 0.87, 0.12, 0.41, 0.59, 0.16, 0.81, 0.65, 0.22, 0.78, 0.06, 0.49, 0.19, 0.73, 0.2, 0.05, 0.73, 0.11, 0.09, 0.29, 0.71, 0.63, 0.16, 0.62, 0.29, 0.12, 0.06, 0.36, 0.13, 0.13, 0.3, 0, 0.7, 0.53, 0.54, 0.81, 0.15, 0.29, 0.65, 0.21, 0.63, 0.45, 0.83, 0.74, 0.74, 0.93, 0.81, 0.35, 0.05, 0.25, 0.41, 0.64, 0.65, 0.62, 0.05, 0.65, 0.83, 0.65, 0.84, 0.6, 0.94, 0.81, 0.72, 0.91, 0.92, 0.78, 0.74, 0.51, 0.96, 0.31, 0.54, 0.23, 0.05, 0.42, 0.47, 0.32, 0.72, 0.69, 0.69, 0.24, 0.3, 0.19, 0.36, 0.58, 0.11, 0.57, 0.97, 0.28, 0.27, 0.88, 0.17, 0.74, 0.34, 0.83, 0.89, 0.42, 0.47, 0.79, 0.51, 0.88, 0.33, 0.85, 0.9, 0.95, 0.72, 0.74, 0.84, 0.88, 0.03, 0.13, 0.49, 0.61, 0.63, 0.24, 0.33, 0.62, 0.21, 0.2, 0.2, 0.49, 0.07, 0.06, 0.36, 0.2, 0.75, 0.56, 0.11, 0.59, 0.21, 0.29, 0.5, 0.02, 0.42, 0.12, 0.65, 0.53, 0.72, 0.72, 0.29, 0.35, 0.51, 0.79, 0.94, 0.26, 0.81, 0.46, 0.83, 0.6, 0.97, 0.81, 0.57, 0.87, 0.04, 0.65, 0.37, 0.32, 0.11, 0.58, 0.78, 0.57, 0.33, 0.43, 0.81, 0.14, 0.38, 0.81, 0.79, 0.7, 0.79, 0.59, 0.5, 0.23, 0.41, 0.27, 0.53, 0.71, 0.93, 0.21, 0.17, 0.64, 0.9, 0.12, 0.03, 0.16, 0.04, 0.8, 0.07, 0.93, 0.26, 0.6, 0.56, 0.43, 0.85, 0.39, 0.18, 0.67, 0.06, 0.99, 0.81, 0.68, 0.62, 0.1, 0.66, 0.95, 0.09, 0.03, 0.01, 0.4, 0.37, 0.36, 0.83, 0.12, 0, 0.45, 0.16, 0.09, 0.84, 0.22, 0.53, 0.6, 0.66, 0.02, 0.03, 0.17, 0.54, 0.55, 0.21, 0.27, 0.38, 0.49, 0.54, 0.5, 0.31, 0.27, 0.75, 0.62, 0.93, 0.59, 0.59, 0.3, 0.51, 0.48, 0.14, 0.45, 0.27, 0.69, 0.36, 0.18, 0.75, 0.3, 0.44, 0.47, 0.14, 0.34, 0.42, 0.71, 0.42, 0.78, 0.43, 0.83, 0.73, 0.31, 0.44, 0.79, 0.08, 0.67, 0.62, 0.09, 0.64, 0.53, 0.45, 0.08, 0.47, 0.28, 0.43, 0.45, 0.28, 0.97, 0.04, 0.42, 0.54, 0.74, 0.85, 0.12, 0.8, 0.84, 0.11, 0.71, 0.84, 0.35, 0.27, 0.17, 0.25, 0.98, 0.05, 0.06, 0.87, 0.47, 0.53, 0.28, 0.26, 0.36, 0.5, 0.66, 0.32, 0.64, 0.1, 0.27, 0.85, 0.43, 0.87, 0.99, 0.99, 0.42, 0.07, 0.53, 0.48, 0.54, 0.27, 0.23, 0.61, 0.11, 0.51, 0.83, 0.01, 0.74, 0.15, 0.8, 0.47, 0.59, 0.01, 0.57, 0.01, 0.48, 0.14, 0.33, 0.9, 0.7, 0.45, 0.68, 0.25, 0.28, 0.25, 0.28, 0.1, 0.69, 0.63, 0.18, 0.34, 0.4, 0.23, 0.13, 0.38, 0.73, 0.78, 0.98, 0.57, 0.92, 0.46, 0.22, 0.94, 0.75, 0.53, 0.6, 0.24, 0.5, 0.3, 1, 0.6, 0.16, 0.78, 0.22, 0.41, 0.33, 0.54, 0.3, 0.42, 0.45, 0.25, 0.45, 0.78, 0.25, 0, 0.11, 0.11, 0.15, 0.88, 0.92, 0.89, 0.48, 0.56, 0.86, 0.1, 0.37, 0.85, 0.8, 0.53, 0.05, 0.52, 0.89, 0.07, 0.49, 0.03, 0.55, 0.51, 0.74, 0.94, 0.78, 0.33, 0.94, 0.23, 0.9, 0.28, 0.34, 0.47, 0.53, 0.71, 0.73, 0.87, 0.76, 0.41, 0.85, 0.33, 0.43, 0.98, 0.19, 0.31, 0.76, 0.21, 0.9, 0.61, 0.51, 0.37, 0.6, 0.4, 0.08, 0.85, 0.47, 0.62, 0.3, 0.49, 0.97, 0.5, 0.93, 0.43, 0.35, 0.62, 0.17, 0.49, 0.59, 0.73, 0.5, 0.85, 0.08, 0.42, 0.85, 0.14, 0.7, 0.51, 0.06, 0.08, 0.53, 0.35, 0.48, 0.84, 0.82, 0.19, 0.43, 0.93, 0.18, 0.52, 0.49, 0.52, 0.09, 0.46, 0.48, 0.45, 0.8, 0.32, 0.88, 0.39, 0.24, 0.37, 0.56, 0.92, 0.75, 0, 0.82, 0.61, 0.33, 0.01, 0.55, 0.18, 0.16, 0.21, 0.56, 0.49, 0.69, 0.52, 0.45, 0.78, 0.17, 0.7, 0.14, 0.57, 0.28, 0.19, 0.15, 0.49, 0.57, 0.32, 0.54, 0.07, 0.81, 0.51, 0.14, 0.08, 0.28, 0.29, 0.24, 0.3, 0.4, 0.82, 0.49, 0.68, 0.48, 0.25, 0.22, 0.58, 0.54, 0.06, 0.97, 0.8, 0.21, 0.53, 0.36, 0.93, 0.54, 0.91, 0.03, 0.04, 0.18, 0.05, 0.97, 0.59, 0.74, 0.09, 0.43, 0.18, 0.33, 0.21, 0.68, 0.4, 0.49, 0.02, 0.31, 0.68, 0.9, 0.37, 0.63, 0.51, 0.65, 0.08, 0.55, 0.61, 0.54, 0.64, 0.92, 0.12, 0.89, 0.21, 0.29, 0.93, 0.86, 0.01, 0.65, 0.77, 0.05, 0.98, 0.72, 0.25, 0.24, 0.41, 0.31, 0.42, 0.62, 0.14, 0.62, 0.45, 0.24, 0.42, 0.59, 0.6, 0.9, 0.16, 0.9, 0.93, 0.16, 0.57, 0.57, 0.31, 0.16, 0.16, 0.03, 0.13, 0.98, 0.47, 0.68, 0.61, 0.28, 0.95, 0.37, 0.62, 0.97, 0.7, 0.29, 0.84, 0.99, 0.06, 0.03, 0.64, 0.43, 0.3, 0.17, 0.08, 0.25, 0.53, 0.95, 0.85, 0.81, 0.01, 0.58, 0.96, 0.56, 0.75, 0.83, 0.17, 0.04, 0.32, 0.67, 0.55, 1, 0.86, 0.22, 0.21, 0.35, 0.56, 0.23, 0.43, 0.87, 0.46, 0.68, 0.15, 0.99, 0.17, 0.46, 0.45, 0.11, 0.6, 0.05, 0.58, 0.93, 0.21, 0.54, 0.14, 0.71, 0.1, 0.98, 0.38, 0.1, 0.46, 0.19, 0.83, 0.31, 0.35, 0.13, 0.12, 1, 0.6, 0.51, 0.33, 0.77, 0.64, 0.55, 0.09, 0.7, 0.02, 0.22, 0.22, 0.93, 0.16, 0.28, 0.93, 0.03, 0.65, 0.18, 0.07, 0.35, 0.39, 0.19, 0.66, 0.65, 0.99, 0.65, 0.66, 0.35, 0.32, 0.98, 0.93, 0.88, 0.51, 0.85, 0.94, 0.79, 0.85, 0.22, 0.06, 0.99, 0.07, 0.15, 0.68, 0.56, 0.55, 0.8, 0.73, 0.39, 0.31, 0.75, 0.6, 0.33, 0.32, 0.51, 0.03, 0.18, 0.81, 0.23, 0.87, 0.66, 0.19, 0.96, 0.31, 0.02, 0.42, 0.53, 0.08, 0.49, 0.19, 0.47, 0.32, 0.11, 0.11, 0.59, 0.12, 0.38, 0.56, 0.4, 0.51, 0.23, 0.53, 0.17, 0.35, 0.6, 0.45, 0.15, 0.29, 0.76, 0.07, 0.54, 0.12, 0.46, 0.68, 0.02, 0.08, 0.36, 0.54, 0.91, 0.58, 0.13, 0.77, 0.34, 0.4, 0.23, 0.44, 0.39, 0.14, 0.01, 0.88, 0.42, 0.48, 0.6, 0.91, 0.48, 0.96, 0.17, 0.37, 0.07, 0.36, 0.73, 0.38, 0.47, 0.98, 0.69, 0.09, 0.53, 0.02, 0.95, 0.77, 0.47, 0.31, 0.49, 0.74, 0.25, 0.04, 0.56, 0.29, 0.56, 0.48, 0.81, 0.03, 0.87, 0.15, 0.43, 0.43, 0.63, 0.3, 0.95, 0.66, 0.41, 0.53, 0.47, 0.31, 0.32, 0.98, 0.19, 0.73, 0.22, 0.94, 0.27, 0.98, 0.84, 0.32, 0.67, 0.5, 0.58, 0.72, 0.42, 0.85, 0.25, 0.31, 0.2, 0.75, 0.16, 0.89, 0.48, 0.75, 0.77, 0.31, 0.95, 0.55, 0.21, 0.66, 0.34, 0.62, 0.59, 0.56, 0.86, 0.23, 0.38, 0.64, 0.43, 0.84, 0.05, 0.29, 0.6, 0.31, 0.94, 0.5, 0.32, 0.1, 0.25, 0.78, 0.66, 0.67, 0.7, 0.5, 0.48, 0.8, 0.37, 0.85, 0.53, 0.61, 0.86, 0.8, 0.89, 0.22, 0.45, 0.53, 0.62, 0.16, 0.99, 0.1, 0.43, 0.92, 0.66, 0.8, 0.2, 0.03, 0.26, 0.07, 0.21, 0.83, 0.6, 0.09, 0.15, 0.67, 0.99, 0.03, 0.05, 0.52, 0.45, 0, 0.37, 0.11, 1, 0.1, 0.14, 0.84, 0.37, 0.55, 0.53, 0.76, 0.16, 0.6, 0.13, 0.03, 0.78, 0.89, 0.91, 0.13, 0.61, 0.49, 0.23, 0.6, 0.67, 0.53, 0.85, 0.41, 0.66, 0.05, 0.98, 0.38, 0.89, 0.45, 0.19, 0.32, 0.46, 0.58, 0.67, 0.33, 0.07, 0.12, 0.32, 0.94, 0.88, 0.85, 0.63, 0.89, 0.82, 0.82, 0.19, 0.03, 0.69, 0.01, 0.17, 0.47, 0.17, 0.01, 0.12, 0.22, 0.74, 0.67, 0.25, 0.84, 0.04, 0.87, 0.61, 0.03, 0.68, 0.22, 0.8, 0.57, 0.27, 0.31, 0.54, 0.76, 0.61, 0.4, 0.33, 0.11, 0.61, 0.89, 0.56, 0.45, 0.67, 0.06, 0.04, 0.29, 0.06, 0.39, 0.92, 0.87, 0.17, 0.94, 0.93, 0.21, 0.09, 0.52, 0.83, 0.57, 0.89, 0.24, 0.91, 0.69, 0.39, 0.57, 0.71, 0.09, 0.64, 0.03, 0.44, 0.05, 0.31, 0.76, 0.34, 0.81, 0.67, 0.9, 0.56, 0.75, 0.01, 0.54, 0.35, 0.63, 0.15, 0.31, 0.9, 0.26, 0.32, 0.23, 0.18, 0.51, 0.96, 0.51, 0.05, 0.07, 0.39, 0.39, 0.22, 0.63, 0.51, 0.2, 0.43, 0.74, 0.13, 0.88, 0.91, 0.12, 0.46, 0.33, 0.65, 0.63, 0.52, 0.84, 0.83, 0.65, 0.64, 0.76, 0.05, 0.67, 0.62, 0.71, 0.33, 0.47, 0.77, 0.75, 0.05, 0.33, 0.1, 0.38, 0.68, 0.29, 0.81, 0.61, 0.39, 0.19, 0.03, 0.53, 0.53, 0.36, 0.08, 0.5, 0.21, 0.93, 0.39, 0.5, 0.07, 0.32, 0.57, 0.24, 0.82, 0.74, 0.26, 0.73, 0.34, 0.62, 0.91, 0.24, 0.03, 0.22, 0.69, 0.89, 0.7, 0.94, 0.14, 0.52, 0.72, 0.47, 0.89, 0.33, 0.37, 0.22, 0.62, 0.57, 0.28, 0.24, 0.6, 0.75, 0.19, 0.55, 0.35, 0.27, 0.02, 0.46, 0.6, 0.95, 0.59, 0.26, 0.13, 0.14, 0.79, 0.37, 0.41, 0.14, 0.42, 0.56, 0.94, 0.13, 0.63, 0.37, 0.35, 0.28, 0.96, 0.38, 0.98, 0.76, 0.81, 0.22, 0.2, 0.33, 0.27, 0.86, 0.63, 0.33],\n",
        "#   top_k=1,\n",
        "#   include_vectors=True,\n",
        "#   include_metadata=True\n",
        "# )\n"
      ],
      "metadata": {
        "id": "hG8vW_tz4F84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import tiktoken\n",
        "\n",
        "from typing import List, Iterator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "#embeddings model, this can be changed to the embedding model of your choice\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\""
      ],
      "metadata": {
        "id": "iqAbq5QjgzIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "WF_rp3HTi06O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import fitz\n",
        "import io\n",
        "\n",
        "#mistral 7B paper\n",
        "url = \"https://arxiv.org/pdf/2310.06825.pdf\"\n",
        "request = requests.get(url)\n",
        "filestream = io.BytesIO(request.content)\n",
        "with fitz.open(stream=filestream, filetype=\"pdf\") as doc:\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_hp4DhSi5tF",
        "outputId": "417bcf4f-87d4-44f6-92ff-17e71b107d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mistral 7B\n",
            "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
            "Devendra Singh Cha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document embedding and tokenizing"
      ],
      "metadata": {
        "id": "EBrFlzvMjWmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "  text = text.replace(\"\\n\", \" \")\n",
        "  return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
        "\n",
        "def tokenize(text,max_tokens) -> pd.DataFrame:\n",
        "    \"\"\" Function to split the text into chunks of a maximum number of tokens \"\"\"\n",
        "\n",
        "    # Load the cl100k_base tokenizer which is designed to work with the ada-002 model\n",
        "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    df=pd.DataFrame(['0',text]).T\n",
        "    df.columns = ['title', 'text']\n",
        "\n",
        "    # Tokenize the text and save the number of tokens to a new column\n",
        "    df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
        "\n",
        "    # Visualize the distribution of the number of tokens per row using a histogram\n",
        "    # df.n_tokens.hist()\n",
        "\n",
        "    ################################################################################\n",
        "    # Step 8\n",
        "    ################################################################################\n",
        "\n",
        "    shortened = []\n",
        "\n",
        "    # Loop through the dataframe\n",
        "    for row in df.iterrows():\n",
        "\n",
        "        # If the text is None, go to the next row\n",
        "        if row[1]['text'] is None:\n",
        "            continue\n",
        "\n",
        "        # If the number of tokens is greater than the max number of tokens, split the text into chunks\n",
        "        if row[1]['n_tokens'] > max_tokens:\n",
        "            shortened += split_into_many(row[1]['text'], tokenizer, max_tokens)\n",
        "\n",
        "        # Otherwise, add the text to the list of shortened texts\n",
        "        else:\n",
        "            shortened.append(row[1]['text'])\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(shortened, columns=['text'])\n",
        "    df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
        "\n",
        "\n",
        "    df['embeddings'] = df.text.apply(lambda x: get_embedding(x))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_into_many(text: str, tokenizer: tiktoken.Encoding, max_tokens: int = 1024) -> list:\n",
        "    \"\"\" Function to split a string into many strings of a specified number of tokens \"\"\"\n",
        "\n",
        "    # Split the text into sentences\n",
        "    sentences = text.split(' ')\n",
        "\n",
        "    # Get the number of tokens for each sentence\n",
        "    n_tokens = [len(tokenizer.encode(\" \" + sentence))\n",
        "                for sentence in sentences]\n",
        "\n",
        "    chunks = []\n",
        "    tokens_so_far = 0\n",
        "    chunk = []\n",
        "\n",
        "    # Loop through the sentences and tokens joined together in a tuple\n",
        "    for sentence, token in zip(sentences, n_tokens):\n",
        "\n",
        "        chunk.append(sentence)\n",
        "        tokens_so_far += token + 1\n",
        "\n",
        "        # If the number of tokens so far plus the number of tokens in the current sentence is greater\n",
        "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
        "        # the chunk and tokens so far\n",
        "        if tokens_so_far + token > max_tokens:\n",
        "            chunks.append(\" \".join(chunk))\n",
        "            chunk = []\n",
        "            tokens_so_far = 0\n",
        "\n",
        "        # If the number of tokens in the current sentence is greater than the max number of\n",
        "        # tokens, go to the next sentence\n",
        "        # if token > max_tokens:\n",
        "        #     continue\n",
        "\n",
        "\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "mLZzJtxUi8Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = tokenize(text, 100)\n"
      ],
      "metadata": {
        "id": "miYIxhnAjZv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "mIRl8w7MjbJB",
        "outputId": "7bfb18db-f49b-4b6b-8c16-13041d373ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  text  n_tokens  \\\n",
              "0    Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...        72   \n",
              "1    Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut...        71   \n",
              "2    the best open 13B\\nmodel (Llama 2) across all ...        64   \n",
              "3    attention (SWA) to effectively handle sequence...        62   \n",
              "4    benchmarks. Our models are released under the ...        74   \n",
              "..                                                 ...       ...   \n",
              "96   Lacroix, Baptiste Rozière, Naman Goyal, Eric H...        72   \n",
              "97   Albert, Amjad Almahairi, Yasmine Babaei,\\nNiko...        75   \n",
              "98   2023.\\n[27] Ashish Vaswani, Noam Shazeer, Niki...        70   \n",
              "99   2017.\\n[28] Rowan Zellers, Ari Holtzman, Yonat...        73   \n",
              "100  Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu...        68   \n",
              "\n",
              "                                            embeddings  \n",
              "0    [0.0018356768414378166, -0.006518179550766945,...  \n",
              "1    [-0.011681824922561646, 0.009741733781993389, ...  \n",
              "2    [-0.025313355028629303, 0.019875671714544296, ...  \n",
              "3    [-0.038964781910181046, 0.007625683676451445, ...  \n",
              "4    [0.006626145914196968, 0.0074492692947387695, ...  \n",
              "..                                                 ...  \n",
              "96   [-0.011586179956793785, 0.0027407791931182146,...  \n",
              "97   [-0.018207961693406105, -0.002385870786383748,...  \n",
              "98   [-0.013977414928376675, 0.017260609194636345, ...  \n",
              "99   [-0.022492052987217903, -0.01783377304673195, ...  \n",
              "100  [0.002822685055434704, -0.004250966943800449, ...  \n",
              "\n",
              "[101 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-625f6f67-04d6-40b3-b877-d127cda09bb0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>n_tokens</th>\n",
              "      <th>embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n",
              "      <td>72</td>\n",
              "      <td>[0.0018356768414378166, -0.006518179550766945,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut...</td>\n",
              "      <td>71</td>\n",
              "      <td>[-0.011681824922561646, 0.009741733781993389, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the best open 13B\\nmodel (Llama 2) across all ...</td>\n",
              "      <td>64</td>\n",
              "      <td>[-0.025313355028629303, 0.019875671714544296, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>attention (SWA) to effectively handle sequence...</td>\n",
              "      <td>62</td>\n",
              "      <td>[-0.038964781910181046, 0.007625683676451445, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>benchmarks. Our models are released under the ...</td>\n",
              "      <td>74</td>\n",
              "      <td>[0.006626145914196968, 0.0074492692947387695, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Lacroix, Baptiste Rozière, Naman Goyal, Eric H...</td>\n",
              "      <td>72</td>\n",
              "      <td>[-0.011586179956793785, 0.0027407791931182146,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Albert, Amjad Almahairi, Yasmine Babaei,\\nNiko...</td>\n",
              "      <td>75</td>\n",
              "      <td>[-0.018207961693406105, -0.002385870786383748,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>2023.\\n[27] Ashish Vaswani, Noam Shazeer, Niki...</td>\n",
              "      <td>70</td>\n",
              "      <td>[-0.013977414928376675, 0.017260609194636345, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>2017.\\n[28] Rowan Zellers, Ari Holtzman, Yonat...</td>\n",
              "      <td>73</td>\n",
              "      <td>[-0.022492052987217903, -0.01783377304673195, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu...</td>\n",
              "      <td>68</td>\n",
              "      <td>[0.002822685055434704, -0.004250966943800449, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>101 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-625f6f67-04d6-40b3-b877-d127cda09bb0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-625f6f67-04d6-40b3-b877-d127cda09bb0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-625f6f67-04d6-40b3-b877-d127cda09bb0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-46ceef89-99dd-4b41-860d-2cc9f575292d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-46ceef89-99dd-4b41-860d-2cc9f575292d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-46ceef89-99dd-4b41-860d-2cc9f575292d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_20b3ab26-5f4e-4010-93c5-6b5fe93290d2\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_20b3ab26-5f4e-4010-93c5-6b5fe93290d2 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading to Upstash Vector DB"
      ],
      "metadata": {
        "id": "SdTIXPyfoup6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['vector_id'] = df.index\n",
        "df['vector_id'] = df['vector_id'].apply(str)"
      ],
      "metadata": {
        "id": "LVJ7R4Bnjbdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Models a simple batch generator that make chunks out of an input DataFrame\n",
        "class BatchGenerator:\n",
        "\n",
        "\n",
        "    def __init__(self, batch_size: int = 10) -> None:\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    # Makes chunks out of an input DataFrame\n",
        "    def to_batches(self, df: pd.DataFrame) -> Iterator[pd.DataFrame]:\n",
        "        splits = self.splits_num(df.shape[0])\n",
        "        if splits <= 1:\n",
        "            yield df\n",
        "        else:\n",
        "            for chunk in np.array_split(df, splits):\n",
        "                yield chunk\n",
        "\n",
        "    # Determines how many chunks DataFrame contains\n",
        "    def splits_num(self, elements: int) -> int:\n",
        "        return round(elements / self.batch_size)\n",
        "\n",
        "    __call__ = to_batches\n",
        "\n",
        "df_batcher = BatchGenerator(300)"
      ],
      "metadata": {
        "id": "z7i894hFjjHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from upstash_vector import Vector"
      ],
      "metadata": {
        "id": "RZV5y9ADmtwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors=[]"
      ],
      "metadata": {
        "id": "H_JbxYCJmw_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsert content vectors\n",
        "print(\"Uploading vectors to content namespace..\")\n",
        "for batch_df in df_batcher(df):\n",
        "  for i in range(0,len(batch_df)):\n",
        "    vec = Vector(id=batch_df.vector_id[i], vector=batch_df.embeddings[i], metadata={\n",
        "        \"text\": batch_df.text[i]\n",
        "    })\n",
        "\n",
        "    vectors.append(vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNPYngcaj0Gz",
        "outputId": "c3af9e96-45bc-4760-9839-f74f6e0908f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading vectors to content namespace..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.upsert(vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DWysb3FpnK2t",
        "outputId": "536a6f8f-da0d-4622-9511-4fde8bab8def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Success'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying"
      ],
      "metadata": {
        "id": "9GN4H4KrnsfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can search for similar vectors\n",
        "\n",
        "embedding = get_embedding(\"What is Mistral?\")\n",
        "\n",
        "# Search for similar vectors\n",
        "res = index.query(vector=embedding, top_k=5, include_metadata=True)\n",
        "[r.metadata['text'] for r in res]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liDsshztnOF_",
        "outputId": "1fbe929a-1c82-4603-c8b4-6aeb8d406f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms',\n",
              " 'Hugging Face 3 is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B',\n",
              " 'that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more\\naffordable,',\n",
              " 'model, Mistral 7B, demonstrates that\\na carefully designed language model can deliver high performance while maintaining an efficient\\ninference. Mistral 7B outperforms the previous best 13B model (Llama 2, [26]) across all tested\\nbenchmarks, and surpasses the best 34B',\n",
              " '2 7B/13B, and Llama 1 34B4 in different\\ncategories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on\\nmost benchmarks. In particular, Mistral 7B displays a superior performance']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r-482dYPn_Rd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}